{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to preprocess a certain word. ie change upper to lower case and remove punctuation\n",
    "def prepocessword(word):\n",
    "    processedWord = \"\"\n",
    "    word = word.lower()\n",
    "    for i in range(len(word)):\n",
    "        if not (ord(word[i]) < 97 or ord(word[i]) > 122):\n",
    "            processedWord += word[i]\n",
    "    return processedWord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method takes in as input the path to the a data set file and creates a vocabulary of the 10000 most frequent\n",
    "# words. It then stores it in the desired formal at the 'outputpath'\n",
    "\n",
    "def createVocab(inputpath, outputpath):\n",
    "    file = open(inputpath, 'r')\n",
    "    word_freq = {}\n",
    "    for line in file:\n",
    "        words = line.split(' ')\n",
    "        # we want to do some preprocessing, ie remove all punctuations and change everything to lower case\n",
    "        for word in words:\n",
    "            word.strip()\n",
    "            # for a character to be a letter, its ascii value has to be between 97 and 122 (a, z) as we have already\n",
    "            # transformed everything to lower case\n",
    "            processedWord = prepocessword(word)\n",
    "            if processedWord != '':\n",
    "                try:\n",
    "                    word_freq[processedWord] += 1\n",
    "                except:\n",
    "                    word_freq[processedWord] = 1\n",
    "    file.close()\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1])\n",
    "    sorted_word_freq.reverse()\n",
    "    sorted_word_freq = sorted_word_freq[:10000]\n",
    "\n",
    "    file = open(outputpath, 'w')\n",
    "    for i in range(len(sorted_word_freq)):\n",
    "        file.write(sorted_word_freq[i][0] + \" \" + str(i) + \" \" + str(sorted_word_freq[i][1]))\n",
    "        file.write('\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method converts a dataset to the binary bag of words representation\n",
    "\n",
    "def convertToBinaryBagOfWords(pathToWords, pathToDataset, output):\n",
    "    sortedWords = {}\n",
    "    file = open(pathToWords, 'r')\n",
    "    for line in file:\n",
    "        temp = line.split(\" \")\n",
    "        sortedWords[temp[0]] = temp[1].strip()\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    file = open(pathToDataset, 'r')\n",
    "    file2 = open(output, 'w')\n",
    "    writer = csv.writer(file2)\n",
    "    reviews = file.readlines()\n",
    "    for i in range(len(reviews)):\n",
    "        print(i)\n",
    "        # for each review, we map each word to the index of that word in the frequency sorted list\n",
    "        # and we change that value to a 1 (ie the word was present)\n",
    "        # if the word isnt in the list nothing happens\n",
    "        # at the end we will have an array of dimension 10000 with a 1 at every index where the word appears\n",
    "        category = 0\n",
    "        review = reviews[i]\n",
    "        vectorRepresentation = np.zeros(10001)\n",
    "        words = review.split(' ')\n",
    "        for word in words:\n",
    "            temp = word.split('\\t')\n",
    "            if len(temp) > 1:\n",
    "                # this means this word contains the category index\n",
    "                # so we first separate the index from the last word\n",
    "                vectorRepresentation[-1] = temp[1].strip()\n",
    "                word = temp[0].strip()\n",
    "            word = prepocessword(word)\n",
    "            try:\n",
    "                # this try block will fail if the word isn't in the 10000 most used words\n",
    "                vectorRepresentation[int(sortedWords[word])] = 1\n",
    "            except:\n",
    "                pass\n",
    "                # the word is not in the 10000 most frequent words\n",
    "        writer.writerow(vectorRepresentation)\n",
    "    file2.close()\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "# and this method converts the dataset to the frequency bag of words representation\n",
    "def convertToFrequencyBagOfWords(pathToWords, pathToDataset, output):\n",
    "    sortedWords = {}\n",
    "    file = open(pathToWords, 'r')\n",
    "    for line in file:\n",
    "        temp = line.split(\" \")\n",
    "        sortedWords[temp[0]] = temp[1].strip()\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    file = open(pathToDataset, 'r')\n",
    "    file2 = open(output, 'w')\n",
    "    writer = csv.writer(file2)\n",
    "    reviews = file.readlines()\n",
    "    for i in range(len(reviews)):\n",
    "        print(i)\n",
    "        category = 0\n",
    "        # for each review, we map each word to the index of that word in the frequency sorted list\n",
    "        # and we change that value to a 1 (ie the word was present)\n",
    "        # if the word isnt in the list nothing happens\n",
    "        # at the end we will have an array of dimension 10000 with a 1 at every index where the word appears\n",
    "        review = reviews[i]\n",
    "        vectorRepresentation = np.zeros(10001)\n",
    "        words = review.split(' ')\n",
    "        occurence = 0\n",
    "        for word in words:\n",
    "            temp = word.split('\\t')\n",
    "            if len(temp) > 1:\n",
    "                # this means this word contains the category index\n",
    "                # so we first separate the index from the last word\n",
    "                category = temp[1].strip()\n",
    "                word = temp[0].strip()\n",
    "            word = prepocessword(word)\n",
    "            try:\n",
    "                vectorRepresentation[int(sortedWords[word])] += 1\n",
    "                occurence += 1\n",
    "            except:\n",
    "                pass\n",
    "                # the word is not in the 10000 most frequent words\n",
    "        for k in range(len(vectorRepresentation)):\n",
    "            vectorRepresentation[k] = float(vectorRepresentation[k]/occurence)\n",
    "\n",
    "        vectorRepresentation[-1]=category\n",
    "        writer.writerow(vectorRepresentation)\n",
    "    file2.close()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method takes a path to the vocabulary, and a path to reviews and changes every review to a sequence\n",
    "# of ID numbers representing specific words\n",
    "def reviewToIdFormat(pathToDictionary, pathToReviews, output):\n",
    "    sortedWords = {}\n",
    "    file = open(pathToDictionary, 'r')\n",
    "    for line in file:\n",
    "        temp = line.split(\" \")\n",
    "        sortedWords[temp[0]] = temp[1].strip()\n",
    "    file.close()\n",
    "\n",
    "    file = open(pathToReviews, 'r')\n",
    "    file2 = open(output, 'w')\n",
    "    reviews = file.readlines()\n",
    "    for i in range(len(reviews)):\n",
    "        newReview = \"\"\n",
    "        category = \"\"\n",
    "        review = reviews[i]\n",
    "        words = review.split(' ')\n",
    "        occurence = 0\n",
    "        for word in words:\n",
    "            temp = word.split('\\t')\n",
    "            if len(temp) > 1:\n",
    "                # this means this word contains the category index\n",
    "                # so we first separate the index from the last word\n",
    "                category = temp[1].strip()\n",
    "                word = temp[0].strip()\n",
    "            word = prepocessword(word)\n",
    "            try:\n",
    "                newReview += str(sortedWords[word]) + ' '\n",
    "            except:\n",
    "                pass\n",
    "                # the word is not in the 10000 most frequent words\n",
    "        newReview = newReview[:-1] + '\\t' + str(category)\n",
    "        file2.write(newReview)\n",
    "        file2.write('\\n')\n",
    "    file2.close()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper method to load reviews and classes from a csv file\n",
    "# used to load the BBoW and FBoW representations\n",
    "def loadReviewAsVectors(path):\n",
    "    file = open(path, 'r')\n",
    "    reader = csv.reader(file)\n",
    "    reviews = []\n",
    "    classes = []\n",
    "    k = 0\n",
    "    for row in reader:\n",
    "        k = k+1\n",
    "        reviews.append([float(i) for i in row[:-1]])\n",
    "        classes.append(float(row[-1]))\n",
    "    file.close()\n",
    "    return reviews,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2, a) \n",
    "# a random classifier and a majority class classifier\n",
    "\n",
    "def randomClassifier_YELP(pathToTestData):\n",
    "    reviews, classes = loadReviewAsVectors(pathToTestData)\n",
    "    predictions = []\n",
    "    truevalues = []\n",
    "    for i in range(len(reviews)):\n",
    "        prediction = random.randint(1,5)\n",
    "        truevalue = int(classes[i])\n",
    "        predictions.append(prediction)\n",
    "        truevalues.append(truevalue)\n",
    "    return metrics.f1_score(truevalues, predictions, average='macro')\n",
    "\n",
    "\n",
    "def majorityClassifier_YELP(pathToTrainingData, pathToTestData):\n",
    "    # we first want to fetch the most common class from the the training data\n",
    "    # and we will predict each review in the test data to that class\n",
    "    reviews, classes = loadReviewAsVectors(pathToTrainingData)\n",
    "    count = [0, 0, 0, 0, 0]\n",
    "    for value in classes:\n",
    "        count[int(value)-1] += 1\n",
    "    mostCommonClass = count.index(max(count)) + 1\n",
    "    print(\"the most common class was \")\n",
    "    print(mostCommonClass)\n",
    "    print(count)\n",
    "    reviews, classes = loadReviewAsVectors(pathToTestData)\n",
    "    predictions = []\n",
    "    actualClass = []\n",
    "    for value in classes:\n",
    "        actualClass.append(value)\n",
    "        predictions.append(mostCommonClass)\n",
    "    return metrics.f1_score(actualClass, predictions, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method is used to train a Bernoulli Naive Bayes classifier\n",
    "# it takes as input differents path to the data, and a filename to output results\n",
    "# it fits classifiers with different hyperparameters and selects the one that has the best results\n",
    "# it then calculates and returns the training validation and test fmeasure for that specific hyperparameter\n",
    "\n",
    "def fitBernoulliNaiveBayes(pathToTraining,  pathToValdidation, pathToTest, filename):\n",
    "    alphavalues = np.arange(0, 0.01, 0.001)\n",
    "    alphavalues = np.concatenate((alphavalues, np.arange(0.01, 0.5, 0.05)))\n",
    "    alphavalues = np.concatenate((alphavalues, np.arange(0.5, 2, 0.1)))\n",
    "    training_reviews, training_classes = loadReviewAsVectors(pathToTraining)\n",
    "    test_reviews, test_classes = loadReviewAsVectors(pathToTest)\n",
    "    validation_reviews, validation_classes = loadReviewAsVectors(pathToValdidation)\n",
    "    # store the results\n",
    "    file = open(\"NaiveBayesFitting_\" + str(filename) + \".csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"alphaValue\", \"fmeasure\"])\n",
    "    bestAlphaValue = 0\n",
    "    bestfmeasure = 0\n",
    "    for alpha in alphavalues:\n",
    "        print(alpha)\n",
    "        clf = BernoulliNB(alpha=alpha, binarize=None)\n",
    "        clf.fit(training_reviews, training_classes)\n",
    "        predictions = clf.predict(validation_reviews)\n",
    "        fmeasure = metrics.f1_score(validation_classes, predictions, average='macro')\n",
    "        writer.writerow([alpha, fmeasure])\n",
    "        if fmeasure > bestfmeasure:\n",
    "            bestAlphaValue = alpha\n",
    "            bestfmeasure = fmeasure\n",
    "\n",
    "    # now that we have the best alpha value we want to evaluate the performance on the test set\n",
    "    clf = BernoulliNB(alpha=bestAlphaValue, binarize=None)\n",
    "    clf.fit(training_reviews, training_classes)\n",
    "    predictions = clf.predict(test_reviews)\n",
    "    fmeasure3 = metrics.f1_score(test_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(training_reviews)\n",
    "    fmeasure = metrics.f1_score(training_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(validation_reviews)\n",
    "    fmeasure2 = metrics.f1_score(validation_classes, predictions, average='macro')\n",
    "    writer.writerow([bestAlphaValue, fmeasure, fmeasure2, fmeasure3])\n",
    "    file.close()\n",
    "    \n",
    "    return bestAlphaValue, fmeasure, fmeasure2, fmeasure3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is similar method as above, but here we are training Gaussian Naive Bayes\n",
    "# and the hyper parameter to tune is now the 'smoothing' parameter\n",
    "def fitGaussianNaiveBayes(pathToTraining,  pathToValdidation, pathToTest, filename):\n",
    "    smoothing = math.pow(10,-10)\n",
    "    training_reviews, training_classes = loadReviewAsVectors(pathToTraining)\n",
    "    test_reviews, test_classes = loadReviewAsVectors(pathToTest)\n",
    "    validation_reviews, validation_classes = loadReviewAsVectors(pathToValdidation)\n",
    "    # store the results\n",
    "    file = open(\"NaiveBayesFitting_\" + str(filename) + \".csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"alphaValue\", \"fmeasure\"])\n",
    "    bestsmoothingvalue = 0\n",
    "    bestfmeasure = 0\n",
    "    while smoothing < math.pow(10,-1):\n",
    "        print(smoothing)\n",
    "        clf = GaussianNB(var_smoothing=smoothing)\n",
    "        clf.fit(training_reviews, training_classes)\n",
    "        predictions = clf.predict(validation_reviews)\n",
    "        fmeasure = metrics.f1_score(validation_classes, predictions, average='macro')\n",
    "        print(fmeasure)\n",
    "        writer.writerow([smoothing, fmeasure])\n",
    "        if fmeasure > bestfmeasure:\n",
    "            bestsmoothingvalue = smoothing\n",
    "            bestfmeasure = fmeasure\n",
    "        smoothing = smoothing * 10\n",
    "    # now that we have the best alpha value we want to evaluate the performance on the test set\n",
    "    clf = GaussianNB(var_smoothing=bestsmoothingvalue)\n",
    "    clf.fit(training_reviews, training_classes)\n",
    "    predictions = clf.predict(test_reviews)\n",
    "    fmeasure3 = metrics.f1_score(test_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(training_reviews)\n",
    "    fmeasure1 = metrics.f1_score(training_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(validation_reviews)\n",
    "    fmeasure2 = metrics.f1_score(validation_classes, predictions, average='macro')\n",
    "    writer.writerow([fmeasure1, fmeasure2, fmeasure3])\n",
    "    file.close()\n",
    "    \n",
    "    return bestsmoothingvalue, fmeasure1, fmeasure2, fmeasure3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method fits a decision tree classifier to the data provided as parameter\n",
    "# it tunes different hyper parameters: Criterion, Splitter, max_depth and min_sample_leaf\n",
    "\n",
    "def fitDecisionTree(pathToTraining,  pathToValdidation, pathToTest, filename):\n",
    "\n",
    "    training_reviews, training_classes = loadReviewAsVectors(pathToTraining)\n",
    "    test_reviews, test_classes = loadReviewAsVectors(pathToTest)\n",
    "    validation_reviews, validation_classes = loadReviewAsVectors(pathToValdidation)\n",
    "    file = open(\"DecisionTreeFitting_\" + str(filename) + \".csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"criterion\", \"splitter\", \"maxdepth\", \"minSampleLeaf\", \"fmeasure\"])\n",
    "    criterions = [\"gini\", \"entropy\"]\n",
    "    splitters = [\"best\", \"random\"]\n",
    "    max_depths = np.arange(0,20, 5)\n",
    "    max_depths = np.concatenate((max_depths, np.arange(21, 45,2)))\n",
    "    min_sample_leaves = np.arange(1, 5, 1)\n",
    "    bestHyperParameters = []\n",
    "    bestFmeasure = 0\n",
    "    for maxdepth in max_depths:\n",
    "        for minsampleleaf in min_sample_leaves:\n",
    "            for criterion in criterions:\n",
    "                if maxdepth == 0:\n",
    "                    maxdepth = None\n",
    "                for splitter in splitters:\n",
    "                    hyperparameters = [criterion, splitter, maxdepth, minsampleleaf]\n",
    "                    print(hyperparameters)\n",
    "                    clf = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=maxdepth,\n",
    "                                                 min_samples_leaf=minsampleleaf)\n",
    "                    clf.fit(training_reviews, training_classes)\n",
    "                    prediction = clf.predict(validation_reviews)\n",
    "                    fmeasure = metrics.f1_score(validation_classes, prediction, average='macro')\n",
    "                    print(fmeasure)\n",
    "                    hyperparameters.append(fmeasure)\n",
    "                    writer.writerow(hyperparameters)\n",
    "                    if fmeasure > bestFmeasure:\n",
    "                        bestFmeasure = fmeasure\n",
    "                        bestHyperParameters = hyperparameters\n",
    "    # now once we are done we want to compute the fmeasure on test set, training set and validations set\n",
    "    clf = DecisionTreeClassifier(criterion=bestHyperParameters[0], splitter=bestHyperParameters[1],\n",
    "                                 max_depth=bestHyperParameters[2],min_samples_leaf=bestHyperParameters[3])\n",
    "    clf.fit(training_reviews, training_classes)\n",
    "    predictions = clf.predict(test_reviews)\n",
    "    fmeasure3 = metrics.f1_score(test_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(training_reviews)\n",
    "    fmeasure1 = metrics.f1_score(training_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(validation_reviews)\n",
    "    fmeasure2 = metrics.f1_score(validation_classes, predictions, average='macro')    \n",
    "    writer.writerow(bestHyperParameters)\n",
    "    writer.writerow([fmeasure1, fmeasure2, fmeasure3])\n",
    "\n",
    "    file.close()\n",
    "    return fmeasure1, fmeasure2, fmeasure3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method fits a linear kernel SVM to the data provided\n",
    "# it tunes the C 'penalty' parameter\n",
    "\n",
    "def fitLinearSVM(pathToTraining,  pathToValdidation, pathToTest, filename):\n",
    "    training_reviews, training_classes = loadReviewAsVectors(pathToTraining)\n",
    "    test_reviews, test_classes = loadReviewAsVectors(pathToTest)\n",
    "    validation_reviews, validation_classes = loadReviewAsVectors(pathToValdidation)\n",
    "    # store the results\n",
    "    file = open(\"LinearSVMFitting_\" + str(filename) + \".csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"C\", \"validationFmeasure\"])\n",
    "    Cs = [0.001, 0.01, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 10, 50, 100]\n",
    "    bestfmeasure = 0\n",
    "    besthyperparameter = 0\n",
    "    for c in Cs:\n",
    "        hyperparameter = c\n",
    "        print(hyperparameter)\n",
    "        clf = LinearSVC(C=c)\n",
    "        clf.fit(training_reviews, training_classes)\n",
    "        predictions = clf.predict(validation_reviews)\n",
    "        fmeasure = metrics.f1_score(validation_classes, predictions, average='macro')\n",
    "        print(fmeasure)\n",
    "        if fmeasure > bestfmeasure:\n",
    "            bestfmeasure = fmeasure\n",
    "            besthyperparameter = hyperparameter\n",
    "        writer.writerow([besthyperparameter, fmeasure])\n",
    "\n",
    "    # now we want to compute, training, validation and test fmeasure for the hyperparameters that lead us to the best\n",
    "    # results\n",
    "\n",
    "    clf = LinearSVC(C=besthyperparameter)\n",
    "    clf.fit(training_reviews, training_classes)\n",
    "    predictions = clf.predict(training_reviews)\n",
    "    fmeasure1 = metrics.f1_score(training_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(validation_reviews)\n",
    "    fmeasure2 = metrics.f1_score(validation_classes, predictions, average='macro')\n",
    "\n",
    "    predictions = clf.predict(test_reviews)\n",
    "    fmeasure3 = metrics.f1_score(test_classes, predictions, average='macro')\n",
    "\n",
    "    writer.writerow([besthyperparameter,fmeasure1, fmeasure2, fmeasure3])\n",
    "    file.close()\n",
    "    return fmeasure1, fmeasure2, fmeasure3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate the vocab files run this cell\n",
    "\n",
    "createVocab(\"./hwk3_datasets/IMDB-train.txt\", \"IMDBvocabfile.txt\")\n",
    "createVocab(\"./hwk3_datasets/YELP-train.txt\", \"YELPvocabfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate the files containing IDs run this cell \n",
    "\n",
    "#IMDB\n",
    "reviewToIdFormat(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-train.txt\", \"IMDB_train.txt\")\n",
    "reviewToIdFormat(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-valid.txt\", \"IMDB_valid.txt\")\n",
    "reviewToIdFormat(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-test.txt\", \"IMDB_test.txt\")\n",
    "\n",
    "#YELP\n",
    "convertToBinaryBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-train.txt\", \"YELP_train.txt\")\n",
    "convertToBinaryBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-valid.txt\", \"YELP_valid.txt\")\n",
    "convertToBinaryBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-test.txt\", \"YELP_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate the BBoW data files run this cell\n",
    "#IMDB\n",
    "convertToBinaryBagOfWords(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-train.txt\", \"IMDB-train.csv\")\n",
    "convertToBinaryBagOfWords(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-valid.txt\", \"IMDB-valid.csv\")\n",
    "convertToBinaryBagOfWords(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-test.txt\", \"IMDB-test.csv\")\n",
    "\n",
    "#YELP\n",
    "convertToBinaryBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-train.txt\", \"YELP-train.csv\")\n",
    "convertToBinaryBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-valid.txt\", \"YELP-valid.csv\")\n",
    "convertToBinaryBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-test.txt\", \"YELP-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate the FBoW data files run this cell\n",
    "\n",
    "#IMDB\n",
    "convertToFrequencyBagOfWords(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-train.txt\", \"IMDB-train.csv\")\n",
    "convertToFrequencyBagOfWords(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-valid.txt\", \"IMDB-valid.csv\")\n",
    "convertToFrequencyBagOfWords(\"IMDB-vocab.txt\", \"./hwk3_datasets/IMDB-test.txt\", \"IMDB-test.csv\")\n",
    "\n",
    "#YELP\n",
    "convertToFrequencyBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-train.txt\", \"YELP-train.csv\")\n",
    "convertToFrequencyBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-valid.txt\", \"YELP-valid.csv\")\n",
    "convertToFrequencyBagOfWords(\"YELP-vocab.txt\", \"./hwk3_datasets/yelp-test.txt\", \"YELP-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train the classifiers run this cell \n",
    "\n",
    "#Naive Bayes:\n",
    "fitBernoulliNaiveBayes(\"YELP_train_BBoW.csv\", \"YELP_valid_BBoW.csv\", \"YELP_test_BBoW.csv\", \"YELP_BBoW\")\n",
    "fitGaussianNaiveBayes(\"YELP_train_FBoW.csv\", \"YELP_valid_FBoW.csv\", \"YELP_test_FBoW.csv\", \"YELP_FBoW\")\n",
    "fitBernoulliNaiveBayes(\"IMDB_train_BBoW.csv\", \"IMDB_valid_BBoW.csv\", \"IMDB_test_BBoW.csv\", \"IMDB_BBoW\")\n",
    "fitGaussianNaiveBayes(\"IMDB_train_FBoW.csv\", \"IMDB_valid_FBoW.csv\", \"IMDB_test_FBoW.csv\", \"IMDB_FBoW\")\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "fitDecisionTree(\"YELP_train_BBoW.csv\", \"YELP_valid_BBoW.csv\", \"YELP_test_BBoW.csv\", \"YELP_BBoW\")\n",
    "fitDecisionTree(\"YELP_train_FBoW.csv\", \"YELP_valid_FBoW.csv\", \"YELP_test_FBoW.csv\", \"YELP_FBoW\")\n",
    "fitDecisionTree(\"IMDB_train_BBoW.csv\", \"IMDB_valid_BBoW.csv\", \"IMDB_test_BBoW.csv\", \"IMDB_BBoW\")\n",
    "\n",
    "fitDecisionTree(\"IMDB_train_FBoW.csv\", \"IMDB_valid_FBoW.csv\", \"IMDB_test_FBoW.csv\", \"IMDB_FBoW\")\n",
    "\n",
    "# Linear SVM\n",
    "\n",
    "fitLinearSVM(\"YELP_train_BBoW.csv\", \"YELP_valid_BBoW.csv\", \"YELP_test_BBoW.csv\", \"YELP_BBoW\")\n",
    "fitLinearSVM(\"IMDB_train_BBoW.csv\", \"IMDB_valid_BBoW.csv\", \"IMDB_test_BBoW.csv\", \"IMDB_BBoW\")\n",
    "fitLinearSVM(\"YELP_train_FBoW.csv\", \"YELP_valid_FBoW.csv\", \"YELP_test_FBoW.csv\", \"YELP_FBoW\")\n",
    "fitLinearSVM(\"IMDB_train_FBoW.csv\", \"IMDB_valid_FBoW.csv\", \"IMDB_test_FBoW.csv\", \"IMDB_FBoW\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
