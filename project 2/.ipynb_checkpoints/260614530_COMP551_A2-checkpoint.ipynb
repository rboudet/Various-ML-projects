{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this method to separate the positive and negative data in training, validation and test sets\n",
    "def separateDataSet(positive, negative):\n",
    "    # and separate the examples in training, validation and test sets\n",
    "    # 60% training, 20% validation, 20% test\n",
    "    trainCutoff = int(len(negative) * 0.6)\n",
    "    validCutoff = int(len(negative) * 0.8)\n",
    "    random.shuffle(negative)\n",
    "    random.shuffle(positive)\n",
    "\n",
    "    negTrain = negative[:trainCutoff]\n",
    "    negValid = negative[trainCutoff:validCutoff]\n",
    "    negTest = negative[validCutoff:]\n",
    "\n",
    "    posTrain = positive[:trainCutoff]\n",
    "    posValid = positive[trainCutoff:validCutoff]\n",
    "    posTest = positive[validCutoff:]\n",
    "\n",
    "    test = np.concatenate([negTest, posTest])\n",
    "    train = np.concatenate([negTrain, posTrain])\n",
    "    validation = np.concatenate([negValid, posValid])\n",
    "    np.random.shuffle(validation)\n",
    "    np.random.shuffle(test)\n",
    "    np.random.shuffle(train)\n",
    "\n",
    "    return train, validation, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this helper method to load the DS1 or DS2 data from the training, validation and test csv files\n",
    "def loadDataSet(number):\n",
    "    train = []\n",
    "    validation = []\n",
    "    test = []\n",
    "    path = \"./DS\" + str(number)\n",
    "    file = open(path + \"_training.csv\", 'r')\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        train.append(row)\n",
    "    file.close()\n",
    "    file = open(path + \"_validation.csv\", 'r')\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        validation.append(row)\n",
    "    file.close()\n",
    "    file = open(path + \"_test.csv\", 'r')\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        test.append(row)\n",
    "    file.close()\n",
    "    train = np.array(train).astype(np.float)\n",
    "    validation = np.array(validation).astype(np.float)\n",
    "    test = np.array(test).astype(np.float)\n",
    "\n",
    "    return train, validation, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we generated the data, we use this method to store all the information in csv files\n",
    "def saveData(number, train, validation, test):\n",
    "     # we save the data in a file\n",
    "    file = open(\"DS\" + str(number) + \"_test.csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(len(test)):\n",
    "        writer.writerow(test[i])\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"DS\" + str(number) + \"_validation.csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(len(validation)):\n",
    "        writer.writerow(validation[i])\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"DS\" + str(number) + \"_training.csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(len(train)):\n",
    "        writer.writerow(train[i])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method will perform the Gaussian discriminant analysis on the DS1 or DS2 depending on the number provided\n",
    "# it will use maximum likelihood method to learn parameters\n",
    "def GDA(train, validation, test, number):\n",
    "    # to learn the means, we average over all vectors of that belong to a specific class\n",
    "    LMean0 = np.zeros(len(test[0]) - 1)\n",
    "    LMean1 = np.zeros(len(test[0]) - 1)  # we do not want to keep the tag that we added at the end of the\n",
    "    c0count = 0\n",
    "    c1count = 0\n",
    "\n",
    "    # the way we have set up the training, validation and test data, we know that half of the data points\n",
    "    # belong to one class and the other half to the other class.\n",
    "    # therefore we do not need to train the class probabilities, as we know it will be 0.5 for both\n",
    "    \n",
    "    \n",
    "    for k in range(len(train)):\n",
    "        temp = train[k][:- 1]\n",
    "        if train[k][-1] == 1:\n",
    "            # we check if this is class 0 or class 1\n",
    "            LMean1 += temp\n",
    "            c1count += 1\n",
    "        else:\n",
    "            LMean0 += temp\n",
    "            c0count += 1\n",
    "\n",
    "    LMean1 = np.array(LMean1 / c1count).reshape((len(LMean1), 1))\n",
    "    LMean0 = np.array(LMean0 / c0count).reshape((len(LMean0), 1))\n",
    "\n",
    "    # now that we have the learned means, we can compute the leaned covariance matrix\n",
    "    # for that we need to compute S0 and S1\n",
    "\n",
    "    S0 = np.zeros((len(train[0]) - 1, len(train[0]) - 1))\n",
    "    S1 = np.zeros((len(train[0]) - 1, len(train[0]) - 1))\n",
    "\n",
    "    c1count = 0\n",
    "    c0count = 0\n",
    "    for k in range(len(train)):\n",
    "        temp = train[k][:len(train[k]) - 1].reshape((len(train[k]) - 1, 1))\n",
    "        if train[k][-1] == 1:\n",
    "            diff = temp - LMean1\n",
    "            diff = np.array(diff)\n",
    "            S1 += np.dot(diff, diff.T)\n",
    "            c1count += 1\n",
    "\n",
    "        else:\n",
    "            diff = temp - LMean0\n",
    "            S0 += np.dot(diff, diff.T)\n",
    "            c0count += 1\n",
    "\n",
    "    S0 = S0 / c0count\n",
    "    S1 = S1 / c1count\n",
    "\n",
    "    # so now the learned Cov matrix is 0.5*S0 + 0.5*S1\n",
    "\n",
    "    Lcov = 0.5 * S0 + 0.5 * S1\n",
    "\n",
    "    # now we test the model with the parameters learned\n",
    "    means = [LMean0, LMean1]\n",
    "    falsePos = 0\n",
    "    truePos = 0\n",
    "    falseNeg = 0\n",
    "    trueNeg = 0\n",
    "    for k in range(len(test)):\n",
    "        predicted = 0\n",
    "        result = [0, 0]\n",
    "        realValue = test[k][-1]\n",
    "        temp = test[k][:len(test[k]) - 1].reshape((len(test[k]) - 1, 1))\n",
    "        # we want to check to evaluate the probability that this value belongs to class 0 and the probability that it\n",
    "        # belongs to class 1. And we will select the class that has the highest prob\n",
    "        det = np.linalg.det(2 * math.pi * Lcov)\n",
    "        for i in range(len(means)):\n",
    "            temp1 = temp - means[i]\n",
    "            inv = np.linalg.inv(Lcov)\n",
    "            expValue = -0.5 * np.matmul(temp1.T, np.matmul(inv, temp1))\n",
    "            prob = 1 / math.sqrt(det) * math.exp(expValue)\n",
    "            result[i] = prob\n",
    "\n",
    "        if result[1] > result[0]:\n",
    "            predicted = 1\n",
    "\n",
    "        # depending on what we predicted, and the actual value we will increment specific values\n",
    "        if predicted == 0:\n",
    "            if realValue == 0:\n",
    "                trueNeg += 1\n",
    "            else:\n",
    "                falseNeg += 1\n",
    "        else:\n",
    "            if realValue == 1:\n",
    "                truePos += 1\n",
    "            else:\n",
    "                falsePos += 1\n",
    "\n",
    "    # now we have all the values, we can compute accuracy, precision, recall and F-Measure\n",
    "    accuracy = (truePos + trueNeg) / len(test)\n",
    "    precision = truePos / (truePos + falsePos)\n",
    "    recall = truePos / (truePos + falseNeg)\n",
    "    Fmeasure = 2 * precision*recall / (precision + recall)\n",
    "    LMean1 = LMean1.reshape((len(LMean1)))\n",
    "    LMean0 = LMean0.reshape((len(LMean0)))\n",
    "    file = open(\"DS\" + str(number)+\"_LMeans.csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(LMean0)\n",
    "    writer.writerow(LMean1)\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"DS\" + str(number)+\"_Lcov.csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    for k in range(len(Lcov)):\n",
    "        writer.writerow(Lcov[k])\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return {\"accuracy \": accuracy, \"precision\" : precision, \"recall\" :recall, \"fmeasure\" :Fmeasure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method will perform the K- nearest neighbour algorithm on the data sets provided\n",
    "# we test a number of k values, and store in a csv file the results for each k value\n",
    "# we then select value that lead to the best validation fmeasure, and we test it on the test data\n",
    "# and we return those values\n",
    "\n",
    "\n",
    "def KNN(train, validation, test, number):\n",
    "    # we first calculate the distance from every vector in validation to all the vectors in the training set\n",
    "    # and we store it in the totalDistances array\n",
    "    totalDistances = []\n",
    "    for i in range(len(validation)):\n",
    "        x = validation[i][:-1]\n",
    "        # we want to find the euclidean distance from x to each of the training data points\n",
    "        euclidean_distance = {}\n",
    "        for j in range(len(train)):\n",
    "            sum = 0\n",
    "            temp = train[j][:-1]  # we remove the class label\n",
    "            for index in range(len(temp)):\n",
    "                sum += math.pow(temp[index] - x[index], 2)\n",
    "            sum = math.sqrt(sum)\n",
    "            euclidean_distance[j] = sum\n",
    "        # now we want to find the k points that are the closest to x (ie smallest euclidean distance)\n",
    "        # ie we want to sort the 'euclidean_distance dictionary by value\n",
    "        sortedDistances = sorted(euclidean_distance.items(), key=lambda item: item[1])\n",
    "        # we want to sort on the 2nd value\n",
    "        totalDistances.append(sortedDistances)\n",
    "\n",
    "    # and once we have all those distances, we can perform the prediction, by averaging out the value\n",
    "    # of the k closest vectors\n",
    "\n",
    "    k_values = np.arange(1, 100, 1)\n",
    "    file = open(\"DS\" + str(number) + \"_KNNresult.csv\", 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"k\", \"F_measure\", \"precision\", \"recall\"])\n",
    "    bestfvalue = 0\n",
    "    bestk = 0\n",
    "    for k in k_values:\n",
    "        falseNeg = 0\n",
    "        falsePos = 0\n",
    "        truePos = 0\n",
    "        trueNeg = 0\n",
    "        performance = []\n",
    "        for i in range(len(validation)):\n",
    "            sum = 0\n",
    "            value = validation[i][-1]\n",
    "            for j in range(k):\n",
    "                index = totalDistances[i][j][0]  # we fetch the index of the vector in the train array\n",
    "                sum += train[index][-1]  # and then with the index we get the class of the vector\n",
    "            sum = sum / k\n",
    "\n",
    "            if sum > 0.5:  # if the average across all k closest neighbours is closer to 1, than we predict a value of 1\n",
    "                if value == 1:\n",
    "                    truePos += 1\n",
    "                else:\n",
    "                    falsePos += 1\n",
    "            else:\n",
    "                if value == 0:\n",
    "                    trueNeg += 1\n",
    "                else:\n",
    "                    falseNeg += 1\n",
    "\n",
    "        precision = truePos / (truePos + falsePos)\n",
    "        recall = truePos / (truePos + falseNeg)\n",
    "        fMeasure = 2 * precision * recall / (precision + recall)\n",
    "        writer.writerow([k, fMeasure, precision, recall])\n",
    "\n",
    "        if fMeasure > bestfvalue:\n",
    "            bestfvalue = fMeasure\n",
    "            bestk = k\n",
    "\n",
    "\n",
    "\n",
    "    # now we want to evaluate the performance on the test set with the best k value we found from the validation set\n",
    "    falseNeg = 0\n",
    "    falsePos = 0\n",
    "    truePos = 0\n",
    "    trueNeg = 0\n",
    "    for i in range(len(test)):\n",
    "        value = test[i][-1]\n",
    "        x = test[i][:-1]\n",
    "        # we want to find the euclidean distance from x to each of the training data points\n",
    "        euclidean_distance = {}\n",
    "        for j in range(len(train)):\n",
    "            sum = 0\n",
    "            temp = train[j][:-1]  # we remove the class label\n",
    "            for index in range(len(temp)):\n",
    "                sum += math.pow(temp[index] - x[index], 2)\n",
    "            sum = math.sqrt(sum)\n",
    "            euclidean_distance[j] = sum\n",
    "        # now we want to find the k points that are the closest to x (ie smallest euclidean distance)\n",
    "        # ie we want to sort the 'euclidean_distance dictionary by value\n",
    "        sortedDistances = sorted(euclidean_distance.items(), key=lambda item: item[1])\n",
    "        sum = 0\n",
    "        for j in range(bestk):\n",
    "            # we take the first k values and sum their class labels\n",
    "            sum += train[sortedDistances[j][0]][-1]\n",
    "        sum = sum / bestk\n",
    "\n",
    "        if sum > 0.5:  # if the average across all k closest neighbours is closer to 1, than we predict a value of 1\n",
    "            if value == 1:\n",
    "                truePos += 1\n",
    "            else:\n",
    "                falsePos += 1\n",
    "        else:\n",
    "            if value == 0:\n",
    "                trueNeg += 1\n",
    "            else:\n",
    "                falseNeg += 1\n",
    "\n",
    "    precision = truePos / (truePos + falsePos)\n",
    "    recall = truePos / (truePos + falseNeg)\n",
    "    fmeasure = 2 * precision * recall / (precision + recall)\n",
    "    file.close()\n",
    "\n",
    "    return {\"best_k_value\":bestk, \"precision\" : precision, \"recall\" :recall, \"fmeasure\" :fmeasure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method fetches the data from the data sets provided \n",
    "# it then generates data points using the mean and covariances loaded\n",
    "def Q1():\n",
    "    file = open(\"./hwk2_datasets/DS1_m_0.txt\")\n",
    "    lines = file.readlines()\n",
    "    m0 = lines[0].split(',')\n",
    "    m0 = m0[:len(m0)-1]\n",
    "    m0 = np.array(m0).astype(np.float)\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"./hwk2_datasets/DS1_m_1.txt\")\n",
    "    lines = file.readlines()\n",
    "    m1 = lines[0].split(',')\n",
    "    m1 = m1[:len(m1)-1]\n",
    "    m1 = np.array(m1).astype(np.float)\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"./hwk2_datasets/DS1_Cov.txt\")\n",
    "    lines = file.readlines()\n",
    "    cov = []\n",
    "    for line in lines:\n",
    "        temp = line.split(',')\n",
    "        temp = temp[:len(temp)-1]\n",
    "        cov.append(np.array(temp).astype(np.float))\n",
    "\n",
    "\n",
    "    negative = []\n",
    "    positive = []\n",
    "\n",
    "    neg = np.random.multivariate_normal(m0, cov, 2000)\n",
    "    pos = np.random.multivariate_normal(m1, cov, 2000)\n",
    "\n",
    "    # we add the label that indicates the class in which the example belongs to\n",
    "    for k in range(len(neg)):\n",
    "        negative.append(np.append(neg[k], 0))\n",
    "        positive.append(np.append(pos[k], 1))\n",
    "\n",
    "    # and separate the examples in training, validation and test sets\n",
    "    # 60% training, 20% validation, 20% test\n",
    "    train, validation, test = separateDataSet(positive, negative)\n",
    "    \n",
    "    saveData(1, train, validation, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method is similar to Q1 however we choose different gaussians with certain probabilities when \n",
    "# generating the data\n",
    "def Q4():\n",
    "    c1means = []  # positive\n",
    "    c2means = []  # negative\n",
    "    covMatrices = []\n",
    "    for i in range(3):\n",
    "        file = open(\"./hwk2_datasets/DS2_c1_m\" + str(i+1) + \".txt\")\n",
    "        lines = file.readlines()\n",
    "        mean = (lines[0].split(','))[:-1]\n",
    "        c1means.append(np.array(mean).astype(np.float))\n",
    "        file.close()\n",
    "        file = open(\"./hwk2_datasets/DS2_c2_m\" + str(i+1) + \".txt\")\n",
    "        lines = file.readlines()\n",
    "        mean = (lines[0].split(','))[:-1]\n",
    "        c2means.append(np.array(mean).astype(np.float))\n",
    "        file.close()\n",
    "        file = open(\"./hwk2_datasets/DS2_Cov\"+str(i+1) + \".txt\")\n",
    "        temp = []\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            temp.append(np.array((line.split(','))[:-1]).astype(np.float))\n",
    "        covMatrices.append(temp)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    # now we want to generate sampling data\n",
    "    # 2000 from positive class and 2000 from negative class \n",
    "    # for each point, we select the gaussian mean and covariance matrix with a certain probability\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for i in range(2000):\n",
    "        gaussSelected = 0\n",
    "        rand = np.random.rand()\n",
    "        if rand < 0.42:\n",
    "            gaussSelected = 1\n",
    "        elif rand < 0.90:\n",
    "            gaussSelected = 2\n",
    "        pos = np.random.multivariate_normal(c1means[gaussSelected], covMatrices[gaussSelected], 1)[0]\n",
    "\n",
    "        neg = np.random.multivariate_normal(c2means[gaussSelected], covMatrices[gaussSelected], 1)[0]\n",
    "        pos = np.append(pos, 1)\n",
    "        neg = np.append(neg, 0)\n",
    "        positive.append(pos)\n",
    "        negative.append(neg)\n",
    "\n",
    "    negative = np.array(negative)\n",
    "    positive = np.array(positive)\n",
    "    train, validation, test = separateDataSet(positive, negative)\n",
    "    saveData(2, train, validation, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 \n",
    "Q1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy ': 0.96625, 'precision': 0.9745547073791349, 'recall': 0.9575, 'fmeasure': 0.9659520807061791}\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "train, validation, test = loadDataSet(1)\n",
    "print(GDA(train, validation, test,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_k_value': 33, 'precision': 0.5532544378698225, 'recall': 0.4675, 'fmeasure': 0.5067750677506775}\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "train, validation, test = loadDataSet(1)\n",
    "print(KNN(train, validation, test,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "\n",
    "Q4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy ': 0.5075, 'precision': 0.5081081081081081, 'recall': 0.47, 'fmeasure': 0.4883116883116883}\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "# 1)\n",
    "\n",
    "train, validation, test = loadDataSet(2)\n",
    "print(GDA(train, validation, test,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3)\n",
    "\n",
    "train, validation, test = loadDataSet(2)\n",
    "print(KNN(train, validation, test,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
